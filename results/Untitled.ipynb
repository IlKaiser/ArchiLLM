{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020c550c-4ab5-4f0e-ad1b-8359add84373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocalamo/anaconda3/envs/kul/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Core Python imports\n",
    "import asyncio\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import base64\n",
    "from typing import Dict, Any, List, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Core llama_index imports - wrapped in try/except to handle missing stubs\n",
    "# and different package versions gracefully\n",
    "try:\n",
    "    from llama_index.core import Settings, VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "    from llama_index.core.workflow import Workflow, step, Event, StartEvent, StopEvent\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    from llama_index.llms.anthropic import Anthropic\n",
    "    from llama_index.core.llms import ChatMessage, MessageRole\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Some llama_index imports failed ({e}). Functionality may be limited.\")\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c06805f-b0a2-4771-83fe-50d2853d1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e01b5e3-3f29-47e1-a22a-8151f6c88177",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def build_github_retriever(\n",
    "    github_token: Optional[str],\n",
    "    owner: str = \"microservices-patterns\",\n",
    "    repo: str = \"ftgo-application\",\n",
    "    branch: str = \"master\",\n",
    "    include_dirs: Optional[List[str]] = None,\n",
    "    force_rebuild: bool = False\n",
    "):\n",
    "    \"\"\"Build and return a LlamaIndex retriever for a GitHub repository.\n",
    "\n",
    "    Example usage:\n",
    "        retriever = await build_github_retriever(os.getenv(\"GITHUB_TOKEN\"))\n",
    "        docs = retriever.retrieve(\"Where are the pattern examples for sagas?\")\n",
    "\n",
    "    The function will set Settings.embed_model to OpenAIEmbedding using the\n",
    "    current OPENAI_API_KEY if available.\n",
    "\n",
    "    Args:\n",
    "        github_token: GitHub API token\n",
    "        owner: Repository owner\n",
    "        repo: Repository name\n",
    "        branch: Branch name\n",
    "        include_dirs: List of directories to include (None for all)\n",
    "        force_rebuild: If True, rebuild index even if cached version exists\n",
    "    \"\"\"\n",
    "    global GITHUB_RETRIEVER\n",
    "    if not github_token:\n",
    "        raise ValueError(\"github_token is required to read private or API-rate-limited repos\")\n",
    "        \n",
    "    # Setup cache directory\n",
    "    cache_dir = Path(\".cache/github_indexes\")\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    persist_dir = cache_dir / f\"{owner}_{repo}_{branch}\"\n",
    "    \n",
    "    # Try to load cached index if it exists and force_rebuild is False\n",
    "    if not force_rebuild and persist_dir.exists():\n",
    "        try:\n",
    "            print(f\"Loading cached index from {persist_dir}\")\n",
    "            # Ensure embeddings are configured\n",
    "            openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if openai_api_key:\n",
    "                Settings.embed_model = OpenAIEmbedding(api_key=openai_api_key)\n",
    "            # Load the index from disk using the storage context\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=str(persist_dir))\n",
    "            index = load_index_from_storage(storage_context)\n",
    "            retriever = index.as_retriever(similarity_top_k=5)\n",
    "            GITHUB_RETRIEVER = retriever\n",
    "            print(\"Successfully loaded cached index\")\n",
    "            return retriever\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load cached index: {e}, rebuilding...\")\n",
    "\n",
    "    github_client = GithubClient(github_token=github_token, verbose=False)\n",
    "\n",
    "    # Default filters: include no specific directories (read whole repo) but exclude binary/docs\n",
    "    filter_directories: Tuple[List[str], GithubRepositoryReader.FilterType]\n",
    "    if include_dirs is None:\n",
    "        # include all directories\n",
    "        filter_directories = ([], GithubRepositoryReader.FilterType.EXCLUDE)\n",
    "    else:\n",
    "        filter_directories = (include_dirs, GithubRepositoryReader.FilterType.INCLUDE)\n",
    "\n",
    "    filter_file_extensions = (\n",
    "        [\n",
    "            \".png\",\n",
    "            \".jpg\",\n",
    "            \".jpeg\",\n",
    "            \".gif\",\n",
    "            \".svg\",\n",
    "            \".ico\",\n",
    "            \"json\",\n",
    "            \".ipynb\",\n",
    "        ],\n",
    "        GithubRepositoryReader.FilterType.EXCLUDE,\n",
    "    )\n",
    "\n",
    "    reader = GithubRepositoryReader(\n",
    "        github_client=github_client,\n",
    "        owner=owner,\n",
    "        repo=repo,\n",
    "        use_parser=False,\n",
    "        verbose=True,\n",
    "        filter_directories=filter_directories,\n",
    "        filter_file_extensions=filter_file_extensions,\n",
    "    )\n",
    "\n",
    "    print(f\"Loading repository {owner}/{repo} (branch={branch}) via GitHub API\")\n",
    "    documents = reader.load_data(branch=branch)\n",
    "    print(f\"Loaded {len(documents)} documents from GitHub repository\")\n",
    "\n",
    "    # Ensure embeddings are configured (use OPENAI_API_KEY if set)\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if openai_api_key:\n",
    "        Settings.embed_model = OpenAIEmbedding(api_key=openai_api_key)\n",
    "\n",
    "    # Build an in-memory vector index\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    \n",
    "    # Save the index to disk for future use\n",
    "    try:\n",
    "        print(f\"Saving index to {persist_dir}\")\n",
    "        index.storage_context.persist(persist_dir=str(persist_dir))\n",
    "        print(\"Successfully saved index to disk\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to save index to disk: {e}\")\n",
    "    \n",
    "    retriever = index.as_retriever(similarity_top_k=5)\n",
    "    GITHUB_RETRIEVER = retriever\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d56f9-478e-4abb-bdad-3b00d72e4665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached index from .cache/github_indexes/microservices-patterns_ftgo-application_master\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from .cache/github_indexes/microservices-patterns_ftgo-application_master/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from .cache/github_indexes/microservices-patterns_ftgo-application_master/index_store.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 22:48:21,521 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded cached index\n"
     ]
    }
   ],
   "source": [
    "rtr = await build_github_retriever(\"ghp-...\")  # Replace with your GitHub token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63279165-c6ea-4512-b214-75cd4a058945",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path(\".cache/github_indexes/microservices-patterns_ftgo-application_master/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437796d5-2bf6-475d-bf59-90ce352d47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=cache_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
